#!/usr/bin/env python2.7
# Alpenhorn: announces the products of mountaineering CHIME instruments.

import logging
import sys
import re

# Set up logger.
logging.basicConfig(level = logging.INFO)
log_fmt = logging.Formatter("%(asctime)s %(levelname)s >> %(message)s", \
                            "%b %d %H:%M:%S")
log = logging.getLogger("")
log.setLevel(logging.INFO)
if sys.argv[0] == "/usr/sbin/alpenhornd":
  log_file = logging.FileHandler("/var/log/alpenhorn/alpenhornd.log", 
                                 mode = 'a')
  log_file.setLevel(logging.INFO)
  log_file.setFormatter(log_fmt)
  log.addHandler(log_file)

from ch_util import data_index as di
import datetime
import h5py
import numpy as np
import os
import peewee as pw
import psutil
import socket
import time
from watchdog.observers import Observer  
from watchdog.observers.polling import PollingObserver  
from watchdog.events import FileSystemEventHandler


# Parameters.
lockfile    = ".writing.lock"
max_time_per_node_operation = 300   # Don't let node operations hog time.
min_loop_time               = 60    # Main loop at most every 60 seconds.



# Register Hook to Log Exception
# ==============================

def log_exception(*args):
  log.error("Fatal error!", exc_info=args)


sys.excepthook = log_exception


# Routines for registering files, acquisitions, copies and info in the DB.
# ========================================================================

def add_acq(name, allow_new_inst=True, allow_new_atype=False, comment=None):
    """Add an aquisition to the database.
    """
    ts, inst, atype = di.parse_acq_name(name)

    # Is the acquisition already in the database?
    if di.ArchiveAcq.select(di.ArchiveAcq.id).where(
                     di.ArchiveAcq.name == name).count():
        raise di.AlreadyExists("Acquisition \"%s\" already exists in DB." %
                               name)

    # Does the instrument already exist in the database?
    try:
        inst_rec = di.ArchiveInst.get(di.ArchiveInst.name == inst)
    except pw.DoesNotExist:
        if allow_new_inst:
            di.ArchiveInst.insert(name=inst).execute()
            logging.info("Added new acquisition instrument \"%s\" to DB." %
                         inst)
        else:
            raise di.DataBaseError("Acquisition instrument \"%s\" not in DB." %
                                   inst)

    # Does the archive type already exist in the database?
    try:
        atype_rec = di.AcqType.get(di.AcqType.name == atype)
    except pw.DoesNotExist:
        if allow_new_atype:
            di.AcqType.insert(name=atype).execute()
            logging.info("Added new acquisition type \"%s\" to DB." % atype)
        else:
            raise di.DataBaseError("Acquisition type \"%s\" not in DB." % atype)

    # Giddy up!
    return di.ArchiveAcq.create(name=name, inst=inst_rec, type=atype_rec,
                                comment=comment)


def get_acqcorrinfo_keywords_from_h5(path):
  f = h5py.File(path, "r")
  n_freq = f["/"].attrs["n_freq"][0]
  n_prod = len(f["/"].attrs["chan_indices"])
  try:
    # This works on the 8-channel correlator.
    integration = f["/"].attrs["acq.udp.spf"][0] * \
                  f["/"].attrs["fpga.int_period"][0]
  except:
    # For now, at least, on the GPU correlator, we need to find the integration
    # time by hand. Find the median difference between timestamps.
    dt = np.array([])
    t = f["timestamp"]
    for i in range(1, len(t)):
      ddt = float(t[i][1]) - float(t[i - 1][1]) + (float(t[i][2]) - \
            float(t[i - 1][2])) * 1e-6
      if t[i][2] + 2e-5 < t[i - 1][2]:
        dt = np.append(dt, ddt + 1.0)
      else:
        dt = np.append(dt, ddt)
      integration = np.median(dt)

  f.close()
  return {"integration" : integration,
          "nfreq"       : n_freq,
          "nprod"       : n_prod}


def get_acqhkinfo_keywords(path):
  fullpath = "%s/%s" % (path, di.fname_atmel)
  fp = open(fullpath, "r")
  if fp == None:
    raise Exception("Could not find \"%s\"." % fullpath)

  ret = []
  while 1:
    l = fp.readline()
    if not l:
      break
    if l[0] == "#":
      continue
    if len(l.split()) < 2:
      continue
    name = l.split()[0]
    iid = " ".join(l.split()[1:])
    ret.append({"atmel_id" : iid, "atmel_name" : name})

  fp.close()
  return ret


def get_filecorrinfo_keywords_from_h5(path):
  f = h5py.File(path, "r")
  start_time = f["timestamp"][0][1] + f["timestamp"][0][2] * 1e-6
  finish_time = f["timestamp"][-1][1] + f["timestamp"][-1][2] * 1e-6
  chunk_number, freq_number = di.parse_corrfile_name(os.path.basename(path))
  f.close()
  return {"start_time"   : start_time,
          "finish_time"  : finish_time,
          "chunk_number" : chunk_number,
          "freq_number"  : freq_number}


def get_filehkinfo_keywords_from_h5(path):
  f = h5py.File(path, "r")
  start_time = f["index_map/time"][0]
  finish_time = f["index_map/time"][-1]
  chunk_number, atmel_name = di.parse_hkfile_name(os.path.basename(path))
  f.close()
  return {"start_time"   : start_time,
          "finish_time"  : finish_time,
          "atmel_name"   : atmel_name,
          "chunk_number" : chunk_number}


def import_file(node, root, acq_name, file_name):
  """Import a file into the DB.

  This routine adds the following to the database, if they do not already exist
  (or might be corrupted).
  - The acquisition that the file is a part of.
  - Information on the acquisition, if it is of type "corr".
  - The file.
  - Information on the file, if it is of type "corr".
  - Indicates that the file exists on this node.
  """
  fullpath = "%s/%s/%s" % (root, acq_name, file_name)

  # Skip the file if ch_master.py still has a lock on it.
  if os.path.isfile("%s/%s/.%s.lock" % (root, acq_name, file_name)):
    log.debug("Skipping \"%s\", which is locked by ch_master.py." % fullpath)
    return

  # Parse the path
  try:
    ts, inst, atype = di.parse_acq_name(acq_name)
  except di.Validation:
    log.info("Skipping non-acquisition path %s." % acq_name)
    return

  # Figure out which acquisition this is; add if necessary.
  try:
    acq = di.ArchiveAcq.get(di.ArchiveAcq.name == acq_name)
    log.debug("Acquisition \"%s\" already in DB. Skipping." % acq_name)
  except pw.DoesNotExist:
    acq = add_acq(acq_name)
    log.info("Acquisition \"%s\" added to DB." % acq_name)

  # What kind of file do we have?
  ftype = di.detect_file_type(file_name)
  if ftype == None:
    log.info("Skipping unrecognised file \"%s/%s\"." % (acq_name, file_name))
    return

  # Make sure information about the acquisition exists in the DB.
  if atype == "corr" and ftype.name == "corr":
    if not acq.corrinfos.count():
      try:
        di.CorrAcqInfo.create(acq=acq,
                              **get_acqcorrinfo_keywords_from_h5(fullpath))
        log.info("Added information for correlator acquisition \"%s\" to " \
                 "DB." % acq_name)
      except:
        log.warning("Missing info for acquistion \"%s\": HDF5 datasets " \
                    "empty. Leaving fields NULL." % (acq_name))
        di.CorrAcqInfo.create(acq=acq)
  elif atype == "hk" and ftype.name == "hk":
    for kw in get_acqhkinfo_keywords("%s/%s" % (root, acq_name)):
      if not sum(1 for _ in di.HKAcqInfo.select().\
                            where(di.HKAcqInfo.acq == acq).\
                            where(di.HKAcqInfo.atmel_name == kw["atmel_name"])):
        try:
          di.HKAcqInfo.create(acq=acq, **kw)
          log.info("Added information for housekeeping acquisition \"%s\", "
                   "board %s to DB." % (acq_name, kw["atmel_name"]))
        except:
          log.warning("Missing info for acquisition \"%s\": atmel_id.dat " \
                      "file missing or corrupt. Skipping this acquisition." % \
                      acq_name)
          return

  # Add the file, if necessary.
  try:
    file = di.ArchiveFile.get(di.ArchiveFile.name == file_name,
                              di.ArchiveFile.acq == acq)
    log.debug("File \"%s/%s\" already in DB. Skipping." % (acq_name, file_name))
  except pw.DoesNotExist:
    md5sum = di.md5sum_file(fullpath, cmd_line = True)
    size_b = os.path.getsize(fullpath)
    file = di.ArchiveFile.create(acq=acq, type=ftype, name=file_name, 
                                 size_b=size_b, md5sum=md5sum)
    log.info("File \"%s/%s\" added to DB." % (acq_name, file_name))

  # Register the copy of the file here on the collection server, if (1) it does
  # not exist, or (2) it does exist but has been labelled as corrupt. If (2),
  # check again.
  if not file.copies.where(di.ArchiveFileCopy.node == node).count():
    copy = di.ArchiveFileCopy.create(file=file, node=node, has_file='Y',
                                     wants_file='Y')
    log.info("Registered file copy \"%s/%s\" to DB." % (acq_name, file_name))

  # Make sure information about the file exists in the DB.
  if ftype.name == "corr":
    # Add if (1) there is no corrinfo or (2) the corrinfo is missing.
    if not file.corrinfos.count():
      try:
        di.CorrFileInfo.create(file=file,
                               **get_filecorrinfo_keywords_from_h5(fullpath))
        log.info("Added information for file \"%s/%s\" to DB." %
                 (acq_name, file_name))
      except:
        if not file.corrinfos.count():
          di.CorrFileInfo.create(file=file)
        log.warning("Missing info for file \"%s/%s\": HDF5 datasets " \
                    "empty or unreadable. Leaving fields NULL." %
                    (acq_name, file_name))
    elif not file.corrinfos[0].start_time:
      try:
        i = file.corrinfos[0]
        k = get_filecorrinfo_keywords_from_h5(fullpath)
      except:
        log.debug("Still missing info for file \"%s/%s\".")
      else:
        i.start_time = k["start_time"]
        i.finish_time = k["finish_time"]
        i.chunk_number = k["chunk_number"]
        i.freq_number = k["freq_number"]
        i.save()
        log.info("Added information for file \"%s/%s\" to DB." %
                 (acq_name, file_name))
  if ftype.name == "hk":
    # Add if (1) there is no hkinfo or (2) the hkinfo is missing.
    if not file.hkinfos.count():
      try:
        di.HKFileInfo.create(file=file,
                             **get_filehkinfo_keywords_from_h5(fullpath))
        log.info("Added information for file \"%s/%s\" to DB." %
                 (acq_name, file_name))
      except:
        if not file.corrinfos.count():
          di.HKFileInfo.create(file=file)
        log.warning("Missing info for file \"%s/%s\": HDF5 datasets " \
                    "empty or unreadable. Leaving fields NULL." %
                    (acq_name, file_name))
    elif not file.hkinfos[0].start_time:
      try:
        i = file.hkinfos[0]
        k = get_filehkinfo_keywords_from_h5(fullpath)
      except:
        log.debug("Still missing info for file \"%s/%s\".")
      else:
        i.start_time = k["start_time"]
        i.finish_time = k["finish_time"]
        i.atmel_name = k["atmel_name"]
        i.chunk_number = k["chunk_number"]
        i.save()
        log.info("Added information for file \"%s/%s\" to DB." %
                 (acq_name, file_name))

def update_node(node):
  # Make sure this node is usable.
  if not node.mounted:
    log.debug("Skipping unmounted node \"%s\"." % node.name)
    return
  if node.suspect:
    log.debug("Skipping suspected node \"%s\"." % node.name)

  log.info("Updating node \"%s\"." % (node.name))

  # Update amount of free space.
  x = os.statvfs(node.root)
  avail_gb = float(x.f_bavail) * x.f_bsize * 9.31323e-10
  di.StorageNode.update(avail_gb=avail_gb,
                        avail_gb_last_checked=datetime.datetime.now()).where(
     di.StorageNode.id == node.id).execute()
  log.info("Node \"%s\" has %.2f GB available." % (node.name, avail_gb))

  # Check for file integrety.
  for fcopy in di.ArchiveFileCopy.select().where(
                 di.ArchiveFileCopy.node == node,
                 di.ArchiveFileCopy.has_file == 'M'):
    fullpath = "%s/%s/%s" % (node.root, fcopy.file.acq.name, fcopy.file.name)
    log.info("Checking file \"%s\" on node \"%s\"." % (fullpath, node.name))
    if os.path.exists(fullpath):
      if di.md5sum_file(fullpath) == fcopy.file.md5sum:
        log.info("File is A-OK!")
        di.ArchiveFileCopy.update(has_file = 'Y').where(
           di.ArchiveFileCopy.id == fcopy.id).execute()
      else:
        log.error("File is corrupted!")
        di.ArchiveFileCopy.update(has_file='X').where(
           di.ArchiveFileCopy.id == fcopy.id).execute()
    else:
      log.error("File does not exist!")
      fcopy.has_file == 'N'
      di.ArchiveFileCopy.update(has_file = 'N').where(
         di.ArchiveFileCopy.id == fcopy.id).execute()

  ## Delete files.

  # If we are below the minimum available size, we should consider all files
  # not explicitly wanted (wants_file != 'Y') as candidates for deletion,
  # otherwise only those explicitly marked (wants_file == 'N')
  # Also do not clean on archive type nodes.
  if avail_gb < node.min_avail_gb and node.storage_type != 'A':
    log.info('Hit minimum available space - considering all unwanted files for deletion!')
    dfclause = di.ArchiveFileCopy.wants_file != 'Y'
  else:
    dfclause = di.ArchiveFileCopy.wants_file == 'N'

  # Search db for candidates on this node to delete.
  del_files = di.ArchiveFileCopy.select().where(dfclause,
                                                di.ArchiveFileCopy.node == node,
                                                di.ArchiveFileCopy.has_file == 'Y')

  # Process candidates for deletion
  del_count = 0  # Counter for number of deletions (used to limit number per node update)
  for fcopy in del_files.order_by(di.ArchiveFileCopy.id):

    # Limit number of deletions to 100 per main loop iteration.
    if del_count >= 100:
      break

    # Get all the *other* copies.
    other_copies = fcopy.file.copies.where(di.ArchiveFileCopy.id != fcopy.id)

    # Get the number of copies on archive nodes
    ncopies = other_copies.join(di.StorageNode).where(di.StorageNode.storage_type == 'A').count()

    shortname = "%s/%s" % (fcopy.file.acq.name, fcopy.file.name)
    fullpath = "%s/%s/%s" % (node.root, fcopy.file.acq.name, fcopy.file.name)

    # If at least two other copies we can delete the file.
    if ncopies >= 2:

      # Use transaction such that errors thrown in the os.remove do not leave
      # the database inconsistent.
      with di.database_proxy.transaction():
        os.remove(fullpath)  # Remove the actual file

        fcopy.has_file = 'N'
        fcopy.wants_file = 'N'  # Set in case it was 'M' before
        fcopy.save()  # Update the FileCopy in the database
        
        log.info("Removed file copy: %s" % shortname)

      del_count += 1

    else:
      log.info("Too few backups to delete %s" % shortname)


  # Check for copy requests (only if this node has not hit the maximum size).
  current_size_gb = 0.0  # Set to zero for the moment. This should be replaced
                         # with the actual archive size at somepoint.

  if current_size_gb < node.max_total_gb or node.max_total_gb < 0.0:
    start_time = time.time()
    for req in di.ArchiveFileCopyRequest.select().where(
                  di.ArchiveFileCopyRequest.completed == False).where(
                  di.ArchiveFileCopyRequest.cancelled == False).where(
                  di.ArchiveFileCopyRequest.group_to == node.group):

      # Only proceed if the source file actually exists (and is not corrupted).
      try:
        di.ArchiveFileCopy.get(di.ArchiveFileCopy.file == req.file,
                               di.ArchiveFileCopy.node == req.node_from,
                               di.ArchiveFileCopy.has_file == 'Y')
      except pw.DoesNotExist:
        log.error("Skipping request for %s/%s since it is not available on "
                  "node \"%s\"." % (req.file.acq.name, req.file.name,
                                    req.node_from.name))
        continue

      # Only proceed if the destination file does not already exist.
      try:
        di.ArchiveFileCopy.get(di.ArchiveFileCopy.file == req.file,
                               di.ArchiveFileCopy.node == node,
                               di.ArchiveFileCopy.has_file == 'Y')
        log.info("Skipping request for %s/%s since it already exists on " \
                 "this node (\"%s\"), and updating DB to reflect this." % \
                 (req.file.acq.name, req.file.name, node.name))
        di.ArchiveFileCopyRequest.update(completed=True).where(
                       di.ArchiveFileCopyRequest.file == req.file).where(
                                    di.ArchiveFileCopyRequest.group_to ==
                                    node.group).execute()
        continue
      except pw.DoesNotExist:
        pass

      # Check that there is enough space available.
      if avail_gb * 1e6 < 2.0 * req.file.size_b:
        log.warning("Node \"%s\" is full: not adding datafile \"%s/%s\"." % \
                    (node.name, req.file.acq.name, req.file.name))
        continue

      # Constuct the origin and destination paths.
      from_path = "%s/%s/%s" % (req.node_from.root, req.file.acq.name, 
                                req.file.name)
      if req.node_from.host != node.host:
        from_path = "%s@%s:%s" % (req.node_from.username, 
                                  req.node_from.address, from_path)

      to_path = "%s/%s/" % (node.root, req.file.acq.name)
      if not os.path.isdir(to_path):
        log.info("Creating directory \"%s\"." % to_path)
        os.mkdir(to_path)

      # Giddy up!
      log.info("Rsyncing file \"%s\"." % req.file.name)
      if os.system("rsync -aqzpt --rsync-path=\"ionice -c2 -n2 rsync\" " \
                   "-e \"ssh -q\" %s %s" % (from_path, to_path)):
        # If the copy didn't work, then the remote file may be corrupted.
        log.error("Rsync failed. Marking source file suspect.")
        di.ArchiveFileCopy.update(has_file = 'M').where(
           di.ArchiveFileCopy.file == req.file).execute()
        continue

      # Check integrety.
      md5sum = di.md5sum_file("%s/%s" % (to_path, req.file.name))
      if md5sum == req.file.md5sum:
        log.info("Successfully compared md5sum.")

        # Update the FileCopy (if exists), or insert a new FileCopy
        try:
          fcopy = di.ArchiveFileCopy.select().where(di.ArchiveFileCopy.file == req.file,
                                                    di.ArchiveFileCopy.node == node).get()
          fcopy.has_file = 'Y'
          fcopy.wants_file = 'Y'
          fcopy.save()

        except pw.DoesNotExist:
          di.ArchiveFileCopy.insert(file=req.file, node=node, has_file='Y',
                                    wants_file='Y').execute()

        # Mark any FileCopyRequest for this file as completed
        di.ArchiveFileCopyRequest.update(completed=True).where(
           di.ArchiveFileCopyRequest.file == req.file).where(
           di.ArchiveFileCopyRequest.group_to == node.group).execute()
      else:
        log.error("Error with md5sum check: %s on node \"%s\", but %s on " \
                  "this node, \"%s\"." % (req.file.md5sum, req.node_from.name,
                  md5sum, node.name))
        log.error("Removing file \"%s/%s\"." % (to_path, req.file.name))
        try:
          os.remove("%s/%s" % (to_path, req.file.name))
        except:
          log.error("Could not remove file.")
        continue

        # Since the md5sum failed, the remote file may be corrupted.
        di.ArchiveFileCopy.update(has_file = 'M').where(
           di.ArchiveFileCopy.file == req.file).execute()

      if time.time() - start_time > max_time_per_node_operation:
        break  # Don't hog all the time.
                               



# Watchdog stuff
# ==============

class register(FileSystemEventHandler):
  def __init__(self, node):
    log.info("Registering node \"%s\" for auto_import watchdog." % (node.name))
    self.node = node
    self.root = node.root
    if self.root[-1] == "/":
      self.root = self.root[0:-1]
    super(register, self).__init__()

  def on_modified(self, event):
    # Figure out the parts; it should be ROOT/ACQ_NAME/FILE_NAME
    subpath = event.src_path.replace(self.root + "/", "").split("/")
    if len(subpath) == 2:
      import_file(self.node, self.root, subpath[0], subpath[1])
    return

  def on_moved(self, event):
    # Figure out the parts; it should be ROOT/ACQ_NAME/FILE_NAME
    subpath = event.dest_path.replace(self.root + "/", "").split("/")
    if len(subpath) == 2:
      import_file(self.node, self.root, subpath[0], subpath[1])
    return

  def on_deleted(self, event):
    # For lockfiles: ensure that the file that was locked is added: it is
    # possible that the watchdog notices that a file has been closed before the
    # lockfile is deleted.
    subpath = event.src_path.replace(self.root + "/", "").split("/")
    if len(subpath) == 2:
      if subpath[1][0] == "." and subpath[1][-5:] == ".lock":
        subpath[1] = subpath[1][1:-5]
        import_file(self.node, self.root, subpath[0], subpath[1])





# Main Street
# ===========

if __name__ == "__main__":
  # We need write access to the DB.
  di.connect_database(read_write=True)

  # Get the nodes mounted on this host.
  host = socket.gethostname().split('.')[0]  # Not sure about this splitting
  node_list = []
  for node in di.StorageNode.select().where(di.StorageNode.host == host).where(
                                            di.StorageNode.mounted):
    node_list.append(node)
  if not len(node_list):
    raise Exception("No nodes on this host (\"%s\") registered in the DB!" % 
                    host)

  # If any node has auto_import set, look for new files and add them to the
  # DB. Then set up a watchdog for it.
  obs_list = []
  for node in node_list:
    if node.auto_import:
      log.info("Crawling base directory \"%s\" for new files." % node.root)
      for acq_name, d, f_list in os.walk(node.root):

        # SUPER HACK!! Force alpenhornd to ignore raw directories for the moment.
        if re.search('\_raw$', acq_name):
          "Skipping blacklisted acq: %s" % acq_name
          continue

        for file_name in sorted(f_list):
          import_file(node, node.root, os.path.basename(acq_name), file_name)
      # If it is an NFS mount, then the default Observer() doesn't work.
      # Determine this by seeing if the node name is the same as the node host:
      # not failsafe, but it will do for now.
      if node.host == node.name:
        obs_list.append(Observer())
      else:
        obs_list.append(PollingObserver(timeout=30))
      obs_list[-1].schedule(register(node), node.root, recursive = True)
    else:
      obs_list.append(None)

  # Caveat canem!
  for obs in obs_list:
    if obs:
      obs.start()

  try:
    while True:
      loop_start = time.time()
      for node in di.StorageNode.select().where(di.StorageNode.host == host):
        update_node(node)
      loop_end = time.time()
      loop_time = loop_end - loop_start
      log.info("Main loop execution was %d sec." % loop_time)
      remaining = min_loop_time - loop_time
      if remaining > 1:
        time.sleep(remaining)
  except KeyboardInterrupt:
    for obs in obs_list:
      if obs:
        obs.stop()

for obs in obs_list:
  if obs:
    obs.join()
