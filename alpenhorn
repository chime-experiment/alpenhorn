#!/usr/bin/env python
# Alpenhorn: announce datafile information to the database

import sys
import datetime

from argh import arg, aliases, dispatch_commands
from argh.interaction import confirm
import peewee as pw

from ch_util import data_index as di
from ch_util import ephemeris


def ask_yesno(question):
    answer = raw_input("%s [Y/N] " % question)
    if answer == "y" or answer == "Y":
        return True
    else:
        return False


@arg('node_name', help='node to copy from')
@arg('group_name', help='group to copy to')
@arg('--force', '-f', help='proceed without confirmation')
@arg('--nice', '-n', help='nice level for transfer')
def node2group(node_name, group_name, force=False, nice=0):
    """sync files from a node to another group"""

    # Make sure we connect RW
    di.connect_database(read_write=True)

    try:
        from_node = di.StorageNode.get(name=node_name)
    except pw.DoesNotExist:
        raise Exception("Node \"%s\" does not exist in the DB." % node_name)
    try:
        to_group = di.StorageGroup.get(name=group_name)
    except pw.DoesNotExist:
        raise Exception("Group \"%s\" does not exist in the DB." % group_name)

    copy = di.ArchiveFileCopy.select().where(
        di.ArchiveFileCopy.node == from_node,
        di.ArchiveFileCopy.has_file == 'Y',
        ~(di.ArchiveFileCopy.file <<
          di.ArchiveFile.select().join(di.ArchiveFileCopy).where(
              di.ArchiveFileCopy.node <<
              di.StorageNode.select().where(
                  di.StorageNode.group == to_group),
              di.ArchiveFileCopy.has_file != 'N')))

    if not copy.count():
        print "No files to copy from node %s." % (node_name)
        return

    print "Will request that %d files be copied from node %s to group %s." % \
          (copy.count(), node_name, group_name)
    if not (force or ask_yesno("Do you want to proceed?")):
        print "Aborted."
        return

    sys.stdout.write("Updating DB ")
    for c in copy:
        try:
            req = di.ArchiveFileCopyRequest.get(file=c.file, group_to=to_group,
                                                node_from=from_node)
            di.ArchiveFileCopyRequest.update(nice=nice, completed=False,
                                             n_requests=req.n_requests + 1,
                                             timestamp=datetime.datetime.now()).where(
                di.ArchiveFileCopyRequest.file == c.file,
                di.ArchiveFileCopyRequest.group_to == to_group,
                di.ArchiveFileCopyRequest.node_from == from_node).execute()
        except pw.DoesNotExist:

            di.ArchiveFileCopyRequest.create(file=c.file, group_to=to_group,
                                             node_from=from_node, nice=nice,
                                             completed=False, n_requests=1,
                                             timestamp=datetime.datetime.now())
        sys.stdout.write(".")
        sys.stdout.flush()
    sys.stdout.write("\n")


@aliases('status')
def summary(width=80):
    """give a short summary of the archive status"""
    col1 = 15
    col2 = 6
    col3 = width - col1 - col2 - 10

    print
    print "Summary of Data Index at %s." % datetime.datetime.now()
    hline = "+-%-*s-+-%*s-+-%-*s-+" % (col1, "-" * col1, col2, "-" * col2,
                                       col3, "-" * col3)
    print hline
    print "| %-*s | %*s | %-*s |" % (col1, "Node", col2, "N File", col3,
                                     "Mount Point")
    print hline
    for node in di.StorageNode.select():
        n_file = di.ArchiveFileCopy.select().where(
            di.ArchiveFileCopy.node == node).count()
        if node.mounted:
            mount_point = "%s:%s" % (node.host, node.root)
        else:
            mount_point = "<unmounted>"
        print "| %-*s | %*d | %-*s |" % (col1, node.name, col2, n_file, col3,
                                         mount_point)
    print hline
    print


@arg('node_name', help='name of node to verify (must be local)')
@arg('--md5', help='perform full check against md5sum')
@arg('--fixdb', help='fix up the database to be consistent with reality')
def verify(node_name, md5=False, fixdb=False):
    """verify the archive against the database.
    """

    import os

    try:
        this_node = di.StorageNode.get(di.StorageNode.name == node_name)
    except pw.DoesNotExist:
        print "Specified node does not exist."
        return

    ## Use a complicated query with a tuples construct to fetch everything we need
    ## in a single query. This massively speeds up the whole process versus
    ## fetching all the FileCopy's then querying for Files and Acqs.
    lfiles = di.ArchiveFile.select(di.ArchiveFile.name, di.ArchiveAcq.name, di.ArchiveFile.size_b, di.ArchiveFile.md5sum, di.ArchiveFileCopy.id).join(di.ArchiveAcq).switch(di.ArchiveFile).join(di.ArchiveFileCopy).where(di.ArchiveFileCopy.node == this_node, di.ArchiveFileCopy.has_file == 'Y').tuples()
    nfiles = lfiles.count()

    missing_files = []
    corrupt_files = []

    missing_ids = []
    corrupt_ids = []

    # Try to use progress bar if available
    try:
        from progress.bar import Bar
        lfiles = Bar('Checking files', max=nfiles).iter(lfiles)
    except ImportError:
        pass

    for filename, acqname, filesize, md5sum, fc_id in lfiles:

        filepath = this_node.root + '/' + acqname + '/' + filename

        # Check if file is plain missing
        if not os.path.exists(filepath):
            missing_files.append(filepath)
            missing_ids.append(fc_id)
            continue

        if md5:
            file_md5 = di.md5sum_file(filepath)
            corrupt = (file_md5 != md5sum)
        else:
            corrupt = (os.path.getsize(filepath) != filesize)

        if corrupt:
            corrupt_files.append(filepath)
            corrupt_ids.append(fc_id)
            continue

    if len(missing_files) > 0:
        print
        print "=== Missing files ==="
        for fname in missing_files:
            print fname

    if len(corrupt_files) > 0:
        print
        print "=== Corrupt files ==="
        for fname in corrupt_files:
            print fname

    print
    print "=== Summary ==="
    print "  %i total files" % nfiles
    print "  %i missing files" % len(missing_files)
    print "  %i corrupt files" % len(corrupt_files)
    print

    # Fix up the database by marking files as missing, and marking
    # corrupt files for verification by alpenhornd.
    if fixdb:

        # Make sure we connect RW
        di.connect_database(read_write=True)

        if len(missing_files) > 0  and ask_yesno('Fix missing files'):
            missing_count = di.ArchiveFileCopy.update(has_file='N').where(di.ArchiveFileCopy.id << missing_ids).execute()
            print "  %i marked as missing" % missing_count

        if len(corrupt_files) > 0  and ask_yesno('Fix corrupt files'):
            corrupt_count = di.ArchiveFileCopy.update(has_file='M').where(di.ArchiveFileCopy.id << corrupt_ids).execute()
            print "  %i corrupt files marked for verification" % corrupt_count
        


@arg('node_name', help='name of node to clean')
@arg('--days', '-d', help='clean files older than <days>')
@arg('--force', '-f', help='force cleaning on an archive node')
def clean(node_name, days=21, force=False):
    """clean up node by marking older files as potentially removable."""

    import peewee as pw
    di.connect_database(read_write=True)

    try:
        this_node = di.StorageNode.get(di.StorageNode.name == node_name)
    except pw.DoesNotExist:
        print "Specified node does not exist."

    # Check to see if we are on an archive node
    if this_node.storage_type == 'A':
        if force:
            print "%s is an archive node. Forcing clean." % node_name
        else:
            print "Cannot clean archive node %s without forcing." % node_name
            return

    oldest = datetime.datetime.now() - datetime.timedelta(days)
    oldest_unix = ephemeris.ensure_unix(oldest)

    ## List of filetypes we want to update, needs a human readable name and a FileInfo table
    filetypes = [ ['correlation',  di.CorrFileInfo],
                  ['housekeeping', di.HKFileInfo] ]

    # Iterate over file types for cleaning
    for name, infotable in filetypes:

        # Select FileCopys on this node.
        oldfiles = di.ArchiveFileCopy.select().where(di.ArchiveFileCopy.node == this_node,
                                                     di.ArchiveFileCopy.wants_file == 'Y')
        # Filter to fetch only ones with a start time older than `oldest`
        oldfiles = oldfiles.join(di.ArchiveFile).join(infotable).where(infotable.start_time < oldest_unix)

        # Get number of correlation files
        count = oldfiles.count()

        if count > 0:
            size_gb = oldfiles.aggregate(pw.fn.Sum(di.ArchiveFile.size_b)) / 2**30.0

            print "Cleaning up %i %s files (%1f GB) from %s " % \
                  (count, name, size_gb, node_name)
            if force or ask_yesno("  Are you sure?"):
                print "  Marking files for cleaning."
                for of in oldfiles:
                    of.wants_file = 'M'
                    of.save()
            else:
                print "  Cancelled"
        else:
            print "No %s files selected for cleaning on %s." % (name, node_name)



if __name__ == "__main__":
    dispatch_commands([node2group, summary, verify, clean])
